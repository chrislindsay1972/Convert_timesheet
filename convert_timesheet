void automation.Timesheet_excel_translation(String file_name,String public_download_url,String source_file_id)
{
out = Map();
out.put("status","ERROR");
out.put("message","Uninitialized.");
info "=== Start Timesheet_excel_translation ===";
// Target WorkDrive folder for output CSVs
TARGET_WORKDRIVE_FOLDER_ID = "7812m0b9bcf5080f24c228adc5c9c0eb4807d";
// -----------------------------------------
// File name & extension
// -----------------------------------------
if(file_name == null || file_name.trim() == "")
{
	file_name = "document.pdf";
}
info "Preferred file name: " + file_name;
ext = "";
dotpos = file_name.lastIndexOf(".");
if(dotpos != -1 && dotpos < file_name.length() - 1)
{
	ext = file_name.substring(dotpos + 1).toLowerCase();
}
info "Detected file extension: " + ext;
if(source_file_id == null)
{
	source_file_id = "";
}
info "WorkDrive source_file_id: " + source_file_id;
if(source_file_id.trim() == "")
{
	info "ERROR: No WorkDrive file id provided.";
	out.put("status","ERROR");
	out.put("message","No WorkDrive file id provided.");
	info out;
	return;
}
// -----------------------------------------
// Supported extensions
// -----------------------------------------
if(ext != "csv" && ext != "pdf")
{
	info "ERROR: Unsupported extension for this function: " + ext;
	out.put("status","ERROR");
	out.put("message","Only CSV and PDF files are supported by Timesheet_excel_translation.");
	info out;
	return;
}
// -----------------------------------------
// OpenAI config
// -----------------------------------------
OPENAI_API_KEY = zoho.crm.getOrgVariable("OPENAI_API_KEY");
if(OPENAI_API_KEY == null || OPENAI_API_KEY.trim() == "")
{
	info "OPENAI_API_KEY org variable is missing.";
	out.put("message","OPENAI_API_KEY not configured.");
	info out;
	return;
}
MODEL = zoho.crm.getOrgVariable("OPENAI_RESPONSES_MODEL");
if(MODEL == null || MODEL.trim() == "")
{
	// Try base model first - detailed instructions may be sufficient
	MODEL = "gpt-4.1-mini";
	// Fine-tuned model (use if base model struggles):
	//MODEL = "ft:gpt-4.1-mini-2025-04-14:personal:rpa-v6-general:ClZZ2K2m";
}
info "Using model: " + MODEL;
// =========================================
// STEP 1: DOWNLOAD FILE (Public link logic first)
// =========================================
raw_download = null;
is_workdrive_error = false;
workdrive_err_msg = "";
// ---------- 1A: Try public_download_url route ----------
if(public_download_url == null)
{
	public_download_url = "";
}
link_in = public_download_url.toString().trim();
if(link_in != "")
{
	info "Using public link route to download WorkDrive file.";
	fname = file_name;
	if(fname == null || fname.trim() == "")
	{
		fname = "file";
	}
	fname = fname.trim();
	fname = fname.replaceAll("[^-A-Za-z0-9._@()+ ]"," ");
	fname = fname.replaceAll("_","_");
	fname = fname.replaceAll("[. ]$","");
	if(fname == "")
	{
		fname = "file";
	}
	safe_name = fname;
	info "SafeName=" + safe_name;
	enc_url = encodeUrl(link_in);
	if(enc_url == null || enc_url == "")
	{
		enc_url = "x";
	}
	enc_url = enc_url.replaceAll("\\+"," ");
	resolve_url = "https://www.zohoapis.eu/workdrive/api/v1/public/links?url=" + enc_url;
	if(resolve_url == null || resolve_url.trim() == "")
	{
		resolve_url = "https://www.zohoapis.eu/workdrive/api/v1/public/links?url=x";
	}
	info "GET " + resolve_url;
	wd_pub = null;
	try 
	{
		wd_pub = invokeurl
		[
			url :resolve_url
			type :GET
		];
	}
	catch (e_res)
	{
		info "Public links resolve exception: " + e_res;
	}
	download_url = "https://example.com";
	if(wd_pub != null)
	{
		if(wd_pub.containsKey("data"))
		{
			d = wd_pub.get("data");
			if(d != null && d.containsKey("attributes"))
			{
				a = d.get("attributes");
				if(a != null && a.containsKey("download_url"))
				{
					download_url = a.get("download_url").toString();
				}
				else if(a != null && a.containsKey("url"))
				{
					download_url = a.get("url").toString();
				}
			}
			else if(d != null && d.containsKey("download_url"))
			{
				download_url = d.get("download_url").toString();
			}
		}
		else if(wd_pub.containsKey("download_url"))
		{
			download_url = wd_pub.get("download_url").toString();
		}
		// NEW: use original WorkDrive name for output
		if(wd_pub.containsKey("link_name") && wd_pub.get("link_name") != null)
		{
			orig_name = wd_pub.get("link_name").toString();
			if(orig_name != null && orig_name.trim() != "")
			{
				file_name = orig_name.trim();
				info "Overriding file_name from public link_name: " + file_name;
				ext = "";
				dotpos = file_name.lastIndexOf(".");
				if(dotpos != -1 && dotpos < file_name.length() - 1)
				{
					ext = file_name.substring(dotpos + 1).toLowerCase();
				}
				info "Detected file extension (from link_name): " + ext;
			}
		}
	}
	if(download_url == null || download_url.trim() == "" || download_url == "https://example.com")
	{
		if(link_in == "")
		{
			info "Public link is blank; skipping public route.";
		}
		else
		{
			if(link_in.indexOf("?") == -1)
			{
				download_url = link_in + "?download=1";
			}
			else
			{
				download_url = link_in + "&download=1";
			}
		}
	}
	if(download_url != null && download_url.trim() != "")
	{
		if(download_url.indexOf("?") == -1)
		{
			download_url = download_url + "?directDownload=true";
		}
		else
		{
			download_url = download_url + "&directDownload=true";
		}
		info "GET " + download_url;
		try 
		{
			file_content = invokeurl
			[
				url :download_url
				type :GET
			];
			if(file_content != null)
			{
				raw_download = file_content;
				info "Public download completed successfully (bytes present).";
			}
			else
			{
				info "Public download returned null bytes; will try WorkDrive /content.";
			}
		}
		catch (e_pubdl)
		{
			info "Public download exception: " + e_pubdl;
		}
	}
}
// ---------- 1B: Fallback to WorkDrive /content ----------
if(raw_download == null)
{
	info "Public route did not yield file bytes; falling back to WorkDrive /content via zoho_drive.";
	try 
	{
		raw_download = invokeurl
		[
			url :"https://workdrive.zoho.eu/api/v1/files/" + source_file_id + "/content"
			type :GET
			connection:"zoho_drive"
		];
		info "WorkDrive /content download complete.";
	}
	catch (eC2)
	{
		info "Content endpoint exception (fallback): " + eC2;
	}
}
if(raw_download == null)
{
	info "ERROR: Could not download file bytes from either public link or WorkDrive /content.";
	out.put("status","ERROR");
	out.put("message","Could not download file bytes from WorkDrive.");
	info out;
	return;
}
// =========================================
// STEP 2: INSPECT DOWNLOADED CONTENT
// =========================================
txtpeek = "";
try 
{
	txtpeek = raw_download.toString();
}
catch (ePeek)
{
	txtpeek = "";
}
// Check for JSON WorkDrive error
if(txtpeek != null && txtpeek != "" && txtpeek.trim().startsWith("{"))
{
	info "Downloaded body looks like JSON; checking for WorkDrive 'errors' array...";
	try 
	{
		json_map = txtpeek.toMap();
		if(json_map.containsKey("errors"))
		{
			errs = json_map.get("errors");
			err_title = "";
			err_id = "";
			for each  e_item in errs
			{
				if(e_item.containsKey("title") && e_item.get("title") != null)
				{
					err_title = e_item.get("title").toString();
				}
				if(e_item.containsKey("id") && e_item.get("id") != null)
				{
					err_id = e_item.get("id").toString();
				}
				break;
			}
			workdrive_err_msg = "WorkDrive download error";
			if(err_id != "")
			{
				workdrive_err_msg = workdrive_err_msg + " (" + err_id + ")";
			}
			if(err_title != "")
			{
				workdrive_err_msg = workdrive_err_msg + ": " + err_title;
			}
			info "ERROR: " + workdrive_err_msg;
			out.put("status","ERROR");
			out.put("message",workdrive_err_msg);
			info out;
			return;
		}
	}
	catch (e_json)
	{
		// Not valid JSON, continue
	}
}
// Check for HTML viewer page (for CSV)
if(ext == "csv" && txtpeek != null && txtpeek != "")
{
	head = txtpeek;
	if(head.length() > 200)
	{
		head = head.substring(0,200);
	}
	low = head.toLowerCase();
	if(low.startsWith("<!doctype") || low.startsWith("<html"))
	{
		info "ERROR: Downloaded CSV is actually HTML (likely viewer page).";
		out.put("status","ERROR");
		out.put("message","Downloaded CSV appears to be HTML (viewer page). Check the WorkDrive public link/sharing.");
		info out;
		return;
	}
}
// Check PDF content looks valid PDF (avoid HTML/JSON AND non-PDF bytes)
if(ext == "pdf" && txtpeek != null && txtpeek != "")
{
	head2 = txtpeek.toString();
	if(head2 != null)
	{
		if(head2.length() > 200)
		{
			head2 = head2.substring(0,200);
		}
		low2 = head2.toLowerCase();
		if(low2.startsWith("<!doctype") || low2.startsWith("<html") || low2.startsWith("{") || low2.startsWith("["))
		{
			info "ERROR: Downloaded PDF appears to be HTML/JSON instead of binary PDF.";
			out.put("status","ERROR");
			out.put("message","Downloaded PDF appears to be HTML/JSON, not a real PDF file.");
			info out;
			return;
		}
		// ===== NEW: if it doesn't contain a %PDF header, treat as CSV instead =====
		pdf_marker_index = low2.indexOf("%pdf-");
		if(pdf_marker_index == -1)
		{
			info "WARNING: File extension is 'pdf' but content does not contain %PDF header; treating as CSV instead.";
			ext = "csv";
		}
	}
}
// Prepare content by extension
csv_input_text = "";
b64 = "";
ct = "application/octet-stream";
if(ext == "csv")
{
	ct = "text/plain";
	csv_input_text = if(txtpeek == null,"",txtpeek);
	info "CSV text extracted (length=" + csv_input_text.length() + ").";
	if(csv_input_text.length() > 400)
	{
		info "CSV preview (first up to 400 chars): " + csv_input_text.substring(0,400);
	}
	else
	{
		info "CSV preview (full): " + csv_input_text;
	}
	if(csv_input_text == "")
	{
		info "ERROR: CSV input is empty.";
		out.put("status","ERROR");
		out.put("message","Downloaded CSV is empty.");
		info out;
		return;
	}
}
else if(ext == "pdf")
{
	ct = "application/pdf";
	b64 = zoho.encryption.base64Encode(raw_download);
	if(b64 == null)
	{
		b64 = "";
	}
	if(b64 == "")
	{
		info "ERROR: Base64 encoding of PDF failed.";
		out.put("status","ERROR");
		out.put("message","Base64 encoding of PDF failed.");
		info out;
		return;
	}
	info "PDF base64 length (chars): " + b64.length();
}
instructions = "You are an AI that converts a timesheet CSV file into a structured CSV table for payroll.\n" + "You must ONLY use values that appear in the CSV columns. Do NOT invent any numbers or components.\n" + "\n" + "CRITICAL RULES - READ FIRST\n" + "1. PROCESS EVERY VALID ROW. Do NOT skip any row that has Candidate RefNo, Forename, Surname, and Weekending.\n" + "2. The HOURS columns determine whether to create a line. If hours = 0, do NOT create that line.\n" + "3. NEVER confuse Rate columns with Hours columns. Rate columns contain pay rates, Hours columns contain worked hours.\n" + "4. NEVER use an OT Rate value as an Expenses value. These are completely separate columns.\n" + "5. NEVER swap amount and rate values. Amount = hours worked, Rate = pay rate per hour.\n" + "6. If OT1 Rate = 720 but OT1 Hours = 0, do NOT create any OT1 line. The rate is irrelevant without hours.\n" + "7. If Expenses = 0, do NOT create an Expenses line, even if OT Rate columns have values.\n" + "8. Process rows sequentially. After processing row N, always process row N+1 if it is valid. Never skip rows.\n" + "9. NEVER change, correct, or invent text values. Copy Client Name and Contract JobTitle EXACTLY as they appear in the CSV.\n" + "\n" + "HANDLING REPEATED/DUPLICATE COLUMN HEADERS\n" + "Sometimes spreadsheets contain duplicate column names (author error). Use POSITION-BASED LOGIC:\n" + "- If the same header appears multiple times, pair columns by position order.\n" + "- Example: Headers are [Std Hrs, Rate, OT1 Hrs, OT1 Rate, Std Hrs, Rate, OT1 Hrs, OT1 Rate]\n" + "  - First 'Std Hrs' (position 1) pairs with first 'Rate' (position 2) = first Std Hrs entry\n" + "  - Second 'Std Hrs' (position 5) pairs with second 'Rate' (position 6) = second Std Hrs entry\n" + "  - First 'OT1 Hrs' (position 3) pairs with first 'OT1 Rate' (position 4) = first OT1 entry\n" + "  - Second 'OT1 Hrs' (position 7) pairs with second 'OT1 Rate' (position 8) = second OT1 entry\n" + "- Each paired group with non-zero hours produces a SEPARATE output line.\n" + "- The pairing rule: The Nth occurrence of an hours column pairs with the Nth occurrence of the corresponding rate column.\n" + "- For each paired group, apply all the same rules (hours must be > 0, etc.).\n" + "- If there are 2 sets of 'Std Hrs'/'Rate' columns with non-zero values, output 2 separate 'Std Hrs' lines.\n" + "\n" + "GOAL\n" + "- Process the input CSV row by row.\n" + "- For each valid timesheet row, build a set of payroll lines for real, non-zero components that have their own columns: standard hours, overtime bands, and expenses.\n" + "- First remove all zero-valued components. Then create expenses lines. Then create standard and overtime hours lines.\n" + "- Each output line represents exactly one component.\n" + "- Do NOT group, merge or cross-reference different rows. Treat every row independently.\n" + "\n" + "VALID DETAIL ROWS\n" + "- Only process a row as a detail row if ALL of the following are true:\n" + "  1) Candidate RefNo is not empty.\n" + "  2) Candidate Forename is not empty.\n" + "  3) Candidate Surname is not empty.\n" + "  4) The Weekending column for that row is present and not empty.\n" + "- Ignore all other rows (blank rows, totals, summary lines, headers, footers).\n" + "- If a row appears to be a repeated header row (e.g., contains 'Candidate RefNo' as a value), skip it.\n" + "- Treat every valid row independently. Do not merge or group rows.\n" + "\n" + "OUTPUT FORMAT\n" + "- Output plain CSV text, DATA ROWS ONLY (no header row).\n" + "- Each output row must have exactly 8 comma-separated values in this order:\n" + "  employeeid,firstname,surname,description,amount,rate,weekending,unit\n" + "- Never output any lines starting with ERROR.\n" + "- Never output comments, explanations or blank lines.\n" + "\n" + "COMMON FIELD MAPPING\n" + "- employeeid  = value from Candidate RefNo.\n" + "- firstname   = value from Candidate Forename.\n" + "- surname     = value from Candidate Surname.\n" + "- client name = value from Client Name.\n" + "- job title   = value from Contract JobTitle.\n" + "- weekending  = value from the Weekending column on that row, converted from DD/MM/YYYY or DD/MM/YY into YYYY-MM-DD.\n" + "\n" + "WHITESPACE HANDLING\n" + "- Trim leading and trailing whitespace from all field values.\n" + "- In the description field, use exactly ONE space before and after each hyphen separator.\n" + "- Collapse multiple consecutive spaces into a single space.\n" + "- Format: 'Type - Client Name - Job Title' with single spaces around hyphens.\n" + "- Example: 'Std Hrs - Acme Corp - Engineer' (not 'Std Hrs -  Acme Corp  - Engineer').\n" + "\n" + "NUMERIC HANDLING\n" + "- For any numeric column (hours, rates, expenses):\n" + "  - Treat empty cells as 0.\n" + "  - Remove thousands separators. For example, 1,088.72 becomes 1088.72.\n" + "  - Keep positive and negative signs as they appear.\n" + "\n" + "IDENTIFYING COMPONENT COLUMNS (CRITICAL - READ CAREFULLY)\n" + "Hours columns and Rate columns are SEPARATE. Never confuse them.\n" + "\n" + "HOURS COLUMNS (these provide the 'amount' field - the number of hours worked):\n" + "- Standard hours column: name contains 'Std Hrs' or 'Std HRs' (case-insensitive).\n" + "- OT1 hours column: name contains 'OT1 Hrs' OR 'OT1 HR' OR 'OT1 HRs' (case-insensitive). This is NOT 'OT1 Rate'.\n" + "- OT2 hours column: name contains 'OT2 Hrs' OR 'OT2Hrs' OR 'OT2 HR' (case-insensitive). This is NOT 'OT2 Rate'.\n" + "- OT3 hours column: name contains 'OT3 Hrs' OR 'OT3 HR' (case-insensitive). This is NOT 'OT3 Rate'.\n" + "\n" + "RATE COLUMNS (these provide the 'rate' field - the pay rate per hour):\n" + "- Standard rate column: name contains 'Std Rate', OR a column named exactly 'Rate' (not 'OT1 Rate', 'OT2 Rate', etc.).\n" + "- OT1 rate column: name is exactly 'OT1 Rate' or contains 'OT1 Rate'.\n" + "- OT2 rate column: name is exactly 'OT2 Rate' or contains 'OT2 Rate'.\n" + "- OT3 rate column: name is exactly 'OT3 Rate' or contains 'OT3 Rate'.\n" + "\n" + "EXPENSES COLUMN:\n" + "- Expenses column: name contains 'Expenses' (and NOT 'Rate').\n" + "- IMPORTANT: OT Rate columns are NOT Expenses. Never use 'OT1 Rate', 'OT2 Rate', 'OT3 Rate' values as expenses.\n" + "\n" + "If a component hours or rate column does NOT exist in the header, treat that component as always zero and never output any line for it.\n" + "\n" + "ROW PROCESSING PIPELINE (APPLY IN THIS ORDER FOR EACH VALID ROW)\n" + "\n" + "STEP 1: READ AND CLEAN VALUES\n" + "- First, scan the header row to identify ALL columns and note if any headers are repeated.\n" + "- If headers are NOT repeated (normal case):\n" + "  - std_hours = value from the Standard hours column (e.g., 'Std Hrs')\n" + "  - std_rate = value from the Standard rate column (e.g., 'Rate' but NOT 'OT1 Rate')\n" + "  - ot1_hours = value from the OT1 hours column (e.g., 'OT1 HR' or 'OT1 Hrs')\n" + "  - ot1_rate = value from the OT1 rate column (e.g., 'OT1 Rate')\n" + "  - ot2_hours = value from the OT2 hours column (e.g., 'OT2Hrs' or 'OT2 Hrs')\n" + "  - ot2_rate = value from the OT2 rate column (e.g., 'OT2 Rate')\n" + "  - ot3_hours = value from the OT3 hours column (e.g., 'OT3 Hrs')\n" + "  - ot3_rate = value from the OT3 rate column (e.g., 'OT3 Rate')\n" + "  - expenses_value = value from the Expenses column ONLY (not from any Rate column)\n" + "- If headers ARE repeated (duplicate column names exist):\n" + "  - Group columns by position: pair the Nth hours column with the Nth rate column of the same type.\n" + "  - Example: If 'Std Hrs' appears at positions 1 and 5, and 'Rate' appears at positions 2 and 6:\n" + "    - Group 1: std_hours_1 = position 1, std_rate_1 = position 2\n" + "    - Group 2: std_hours_2 = position 5, std_rate_2 = position 6\n" + "  - Process each group independently, creating separate output lines for each group with non-zero hours.\n" + "- Convert all values to numbers using the numeric handling rules.\n" + "- After conversion, any missing component should be treated as 0.\n" + "\n" + "STEP 2: DETERMINE WHICH LINES TO CREATE (HOURS MUST BE NON-ZERO)\n" + "- The decision to create a line is based on the HOURS column, not the Rate column:\n" + "  - Create a Std Hrs line ONLY IF std_hours > 0\n" + "  - Create an OT1 Hrs line ONLY IF ot1_hours > 0\n" + "  - Create an OT2 Hrs line ONLY IF ot2_hours > 0\n" + "  - Create an OT3 Hrs line ONLY IF ot3_hours > 0\n" + "  - Create an Expenses line ONLY IF expenses_value > 0\n" + "- CRITICAL: If ot1_hours = 0, do NOT create an OT1 line even if ot1_rate has a value.\n" + "- CRITICAL: If expenses_value = 0, do NOT create an Expenses line even if rate columns have values.\n" + "\n" + "STEP 3: CREATE EXPENSES LINE FIRST\n" + "- ONLY if the Expenses column value is non-zero:\n" + "  - Output exactly one expenses line for this row:\n" + "      description = Expenses - {Client Name} - {Contract JobTitle}\n" + "      amount      = 1\n" + "      rate        = expenses_value (from Expenses column ONLY, never from Rate columns)\n" + "      unit        = expense\n" + "- If Expenses = 0 or empty, do NOT output any expenses line.\n" + "- NEVER use OT1 Rate, OT2 Rate, or OT3 Rate as the expenses value.\n" + "\n" + "STEP 4: CREATE STANDARD AND OVERTIME LINES\n" + "- After creating the expenses line (if any), create lines for non-zero hours components.\n" + "\n" + "4A) STANDARD HOURS\n" + "- If std_hours > 0 AND std_rate > 0:\n" + "  - Output exactly one standard-hours line:\n" + "      description = Std Hrs - {Client Name} - {Contract JobTitle}\n" + "      amount      = std_hours (the hours value, typically a small number like 5, 40, 45)\n" + "      rate        = std_rate (the pay rate, which can be larger like 220, 350, etc.)\n" + "      unit        = hours\n" + "\n" + "4B) OVERTIME 1\n" + "- ONLY IF ot1_hours > 0 AND ot1_rate > 0:\n" + "  - Output exactly one OT1 line:\n" + "      description = OT1 Hrs - {Client Name} - {Contract JobTitle}\n" + "      amount      = ot1_hours (the hours value from OT1 Hours column)\n" + "      rate        = ot1_rate (the rate value from OT1 Rate column)\n" + "      unit        = hours\n" + "- If ot1_hours = 0, do NOT create any OT1 line, regardless of ot1_rate value.\n" + "- NEVER swap amount and rate. Amount is always hours, rate is always the pay rate.\n" + "\n" + "4C) OVERTIME 2\n" + "- ONLY IF ot2_hours > 0 AND ot2_rate > 0:\n" + "  - Output exactly one OT2 line:\n" + "      description = OT2 Hrs - {Client Name} - {Contract JobTitle}\n" + "      amount      = ot2_hours\n" + "      rate        = ot2_rate\n" + "      unit        = hours\n" + "- If ot2_hours = 0, do NOT create any OT2 line.\n" + "\n" + "4D) OVERTIME 3\n" + "- ONLY IF ot3_hours > 0 AND ot3_rate > 0:\n" + "  - Output exactly one OT3 line:\n" + "      description = OT3 Hrs - {Client Name} - {Contract JobTitle}\n" + "      amount      = ot3_hours\n" + "      rate        = ot3_rate\n" + "      unit        = hours\n" + "- If ot3_hours = 0, do NOT create any OT3 line.\n" + "\n" + "STEP 5: VALIDATE EACH OUTPUT LINE\n" + "- After constructing each line, perform a final validation:\n" + "  - Verify the hours column was non-zero for any OT line you created.\n" + "  - Verify amount and rate are not swapped (amount should be hours worked, rate should be pay rate).\n" + "  - Do not output an Expenses line if the Expenses column value was zero.\n" + "- Exclude any line that fails validation.\n" + "\n" + "ONE COMPONENT PER LINE\n" + "- Each output line must represent exactly one component.\n" + "- Never mix hours from different components in the same line.\n" + "- Never reuse a rate from one component for a different component.\n" + "- Valid description prefixes are only:\n" + "  Std Hrs -\n" + "  OT1 Hrs -\n" + "  OT2 Hrs -\n" + "  OT3 Hrs -\n" + "  Expenses -\n" + "\n" + "COMMON ERRORS TO AVOID\n" + "1. Creating an OT1 line when OT1 hours = 0 but OT1 Rate has a value. DO NOT DO THIS.\n" + "2. Using OT1 Rate value as the Expenses value. These are different columns.\n" + "3. Swapping amount and rate (e.g., putting 720 in amount and 1 in rate when it should be the opposite or no line at all).\n" + "4. Creating an Expenses line when Expenses = 0. Only create if Expenses column has a non-zero value.\n" + "5. SKIPPING ROWS. Every valid row MUST produce at least one output line (unless all hours and expenses are zero).\n" + "6. Inconsistent whitespace in descriptions. Always use single spaces around hyphens.\n" + "7. CHANGING OR INVENTING VALUES. Never change, correct, or invent Client Name or Job Title values. Copy them EXACTLY as they appear in the source CSV, character for character.\n" + "\n" + "NET PAY AND DISCREPANCIES\n" + "- If there is a Net pay or Net Pay column, use it only as a cross-check.\n" + "- For each row, the component lines must be based ONLY on the Std, OT and Expenses values in that row.\n" + "- You must NOT change, remove or invent any component to force your totals to match Net pay.\n" + "- If the sum of (amount * rate) for all components does not match the Net pay value, do NOT adjust any hours, rates or expenses to make them match.\n" + "- In case of any discrepancy, always trust the actual Std, OT and Expenses columns and ignore the Net pay value for calculation.\n" + "- Never output a line that represents Net Pay or Total Pay.\n" + "- Ignore Total Pay completely for payroll calculations.\n" + "\n" + "SUMMARY\n" + "- For each valid row:\n" + "  1) Scan headers for duplicates. If duplicates exist, use position-based pairing.\n" + "  2) Read and clean numeric values from the correct columns (hours from hours columns, rates from rate columns, expenses from expenses column).\n" + "  3) Only create a line if the HOURS value is non-zero (for hours-based lines) or EXPENSES value is non-zero (for expenses lines).\n" + "  4) If expenses_value > 0, output exactly one Expenses line with amount=1 and rate=expenses_value.\n" + "  5) Then output Std Hrs, OT1 Hrs, OT2 Hrs, OT3 Hrs lines ONLY for components where hours > 0.\n" + "  6) If duplicate headers created multiple groups, output a separate line for EACH group with non-zero hours.\n" + "  7) Never use OT Rate values as Expenses. Never swap amount and rate.\n" + "\n" + "EXAMPLE WITH DUPLICATE HEADERS\n" + "Input headers: Candidate RefNo, Client Name, Contract JobTitle, ..., Std Hrs, Rate, Std Hrs, Rate, Weekending\n" + "Input row values: CAN-123, Acme Corp, Engineer, ..., 40, 25, 8, 30, 21/12/2025\n" + "Output (2 lines because both Std Hrs groups have non-zero hours):\n" + "  CAN-123,John,Smith,Std Hrs - Acme Corp - Engineer,40,25,2025-12-21,hours\n" + "  CAN-123,John,Smith,Std Hrs - Acme Corp - Engineer,8,30,2025-12-21,hours\n" + "\n" + "CONCRETE EXAMPLES OF INCORRECT OUTPUT - DO NOT DO THIS\n" + "\n" + "EXAMPLE A - WRONG (OT Rate value incorrectly used as Expenses):\n" + "Input row: CAN-158053, Std1 Hrs=45, OT1 Hrs=0, Std Rate=21.4, OT1 Rate=35, Expenses=0\n" + "WRONG OUTPUT (DO NOT PRODUCE):\n" + "  CAN-158053,...,Expenses - McHugh Civil Engineering - 360 Operator,1,35,...,expense  <- WRONG! Expenses=0 in input, 35 is OT1 Rate NOT Expenses\n" + "CORRECT OUTPUT:\n" + "  CAN-158053,...,Std Hrs - McHugh Civil Engineering - 360 Operator,45,21.4,...,hours  <- Only this single line\n" + "\n" + "EXAMPLE B - WRONG (OT1 line created when OT1 Hours=0, with swapped amount/rate):\n" + "Input row: CAN-200801, Std1 Hrs=5, OT1 Hrs=0, Std Rate=480, OT1 Rate=720, Expenses=0\n" + "WRONG OUTPUT (DO NOT PRODUCE):\n" + "  CAN-200801,...,Std Hrs - Kier Integrated Services - Sub Agent,5,480,...,hours\n" + "  CAN-200801,...,OT1 Hrs - Kier Integrated Services - Sub Agent,720,1,...,hours   <- WRONG! OT1 Hrs=0 so NO OT1 line should exist\n" + "CORRECT OUTPUT:\n" + "  CAN-200801,...,Std Hrs - Kier Integrated Services - Sub Agent,5,480,...,hours   <- Only this single line\n" + "\n" + "EXAMPLE C - WRONG (OT Rate treated as hours):\n" + "Input row: CAN-149359, Std1 Hrs=5, OT1 Hrs=0, Std Rate=450, OT1 Rate=675, Expenses=0\n" + "WRONG OUTPUT (DO NOT PRODUCE):\n" + "  CAN-149359,...,OT1 Hrs - ...,675,675,...,hours  <- WRONG! OT1 Hrs=0, the value 675 is a RATE not hours\n" + "CORRECT OUTPUT:\n" + "  CAN-149359,...,Std Hrs - ...,5,450,...,hours    <- Only this single line\n" + "\n" + "KEY INSIGHT FOR AVOIDING THESE ERRORS:\n" + "When you see a large number in the OT1 Rate column (like 35, 675, 720) but OT1 Hrs=0:\n" + "- That large number is IRRELEVANT - it is just a stored rate for future use\n" + "- Do NOT create any OT1 line\n" + "- Do NOT use that number as Expenses\n" + "- Do NOT put that number anywhere in the output\n" + "- Simply IGNORE it completely\n" + "\n" + "VALIDATION CHECKLIST (Apply to every output line you generate):\n" + "[ ] For each OT1/OT2/OT3 line: Did the corresponding OT Hours column have a value > 0?\n" + "[ ] For each Expenses line: Did the Expenses column have a value > 0?\n" + "[ ] Is the amount field a reasonable hours value (typically 0.5 to 70)?\n" + "[ ] Is the rate field the pay rate (can be large like 200, 400, 720)?\n" + "[ ] Did I avoid using any OT Rate value as an Expenses value?\n" + "If any check fails, DELETE that output line.\n";
info "Instructions length (chars): " + instructions.length();
headers = Map();
headers.put("Authorization","Bearer " + OPENAI_API_KEY);
headers.put("Content-Type","application/json");
// =========================================
// STEP 4: Call OpenAI (CSV vs PDF)
// =========================================
// ---------- CSV FLOW ----------
if(ext == "csv")
{
	info "Branch selected: CSV";
	info "CSV input length (chars): " + csv_input_text.length();
	// --- Split into header + data rows using toList (no join/split) ---
	rows_all = csv_input_text.toList("\n");
	if(rows_all == null || rows_all.size() == 0)
	{
		info "ERROR: Downloaded CSV has no lines.";
		out.put("status","ERROR");
		out.put("message","Downloaded CSV has no lines.");
		info out;
		return;
	}
	header_row = rows_all.get(0);
	data_rows = List();
	row_index = 0;
	for each  r_all in rows_all
	{
		if(row_index > 0)
		{
			data_rows.add(r_all);
		}
		row_index = row_index + 1;
	}
	if(data_rows.size() == 0)
	{
		info "ERROR: CSV has only a header row and no data rows.";
		out.put("status","ERROR");
		out.put("message","Downloaded CSV has only a header row and no data rows.");
		info out;
		return;
	}
	// --- Line-based chunking (header + up to N data rows per chunk) ---
	CHUNK_MAX_LINES = 10;
	// e.g. 80 data rows per chunk
	chunk_texts = List();
	current_chunk = "";
	current_line_count = 0;
	for each  drow in data_rows
	{
		if(current_chunk == "")
		{
			// start a new chunk with header + first data row
			current_chunk = header_row + "\n" + drow;
			current_line_count = 1;
		}
		else
		{
			if(current_line_count >= CHUNK_MAX_LINES)
			{
				// close current chunk and start a new one
				chunk_texts.add(current_chunk);
				current_chunk = header_row + "\n" + drow;
				current_line_count = 1;
			}
			else
			{
				// append data row to current chunk
				current_chunk = current_chunk + "\n" + drow;
				current_line_count = current_line_count + 1;
			}
		}
	}
	if(current_chunk != "")
	{
		chunk_texts.add(current_chunk);
	}
	num_chunks = chunk_texts.size();
	info "CSV will be processed in " + num_chunks + " chunk(s) with max " + CHUNK_MAX_LINES + " data rows each.";
	// --- Call OpenAI for each chunk and merge results ---
	merged_output = "";
	chunk_index = 0;
	for each  csv_chunk_text in chunk_texts
	{
		chunk_index = chunk_index + 1;
		info "Processing CSV chunk " + chunk_index + " / " + num_chunks + " (chars=" + csv_chunk_text.length() + ").";
		part_text = Map();
		part_text.put("type","input_text");
		part_text.put("text",instructions);
		part_csv = Map();
		part_csv.put("type","input_text");
		part_csv.put("text",csv_chunk_text);
		content_list = List();
		content_list.add(part_text);
		content_list.add(part_csv);
		user_msg = Map();
		user_msg.put("role","user");
		user_msg.put("content",content_list);
		input_items = List();
		input_items.add(user_msg);
		body_map = Map();
		body_map.put("model",MODEL);
		body_map.put("input",input_items);
		//body_map.put("temperature",0);
		info "About to call OpenAI Responses API for CSV chunk " + chunk_index + "...";
		resp_csv = null;
		try 
		{
			resp_csv = invokeurl
			[
				url :"https://api.openai.com/v1/responses"
				type :POST
				body:body_map.toString()
				headers:headers
			];
			info "Responses API call complete for CSV chunk " + chunk_index + ".";
		}
		catch (e_resp_csv)
		{
			info "Responses API exception for CSV chunk " + chunk_index + ": " + e_resp_csv;
			out.put("status","ERROR");
			out.put("message","Responses API exception (CSV chunk " + chunk_index + "): " + e_resp_csv);
			info out;
			return;
		}
		if(resp_csv == null)
		{
			info "ERROR: Empty response from OpenAI for CSV chunk " + chunk_index + ".";
			out.put("status","ERROR");
			out.put("message","Empty response from OpenAI for CSV chunk " + chunk_index + ".");
			info out;
			return;
		}
		resp_text_csv = resp_csv.toString();
		if(resp_text_csv.length() > 500)
		{
			info "Responses API raw (CSV chunk " + chunk_index + ", first 500 chars): " + resp_text_csv.substring(0,500);
		}
		else
		{
			info "Responses API raw (CSV chunk " + chunk_index + ", full): " + resp_text_csv;
		}
		// --- Parse OpenAI response for this chunk ---
		chunk_output_single = "";
		try 
		{
			rm_csv = resp_text_csv.toMap();
			// error object
			if(rm_csv != null && rm_csv.containsKey("error") && rm_csv.get("error") != null)
			{
				err_obj_c = rm_csv.get("error");
				err_msg_c = "";
				err_type_c = "";
				err_code_c = "";
				if(err_obj_c != null)
				{
					if(err_obj_c.containsKey("message") && err_obj_c.get("message") != null)
					{
						err_msg_c = err_obj_c.get("message").toString();
					}
					if(err_obj_c.containsKey("type") && err_obj_c.get("type") != null)
					{
						err_type_c = err_obj_c.get("type").toString();
					}
					if(err_obj_c.containsKey("code") && err_obj_c.get("code") != null)
					{
						err_code_c = err_obj_c.get("code").toString();
					}
				}
				full_err_c = err_msg_c;
				if(err_type_c != "")
				{
					full_err_c = full_err_c + " (type: " + err_type_c + ")";
				}
				if(err_code_c != "")
				{
					full_err_c = full_err_c + " [code: " + err_code_c + "]";
				}
				if(full_err_c == "" || full_err_c == null)
				{
					full_err_c = resp_text_csv;
				}
				info "OpenAI error detected for CSV chunk " + chunk_index + ": " + full_err_c;
				out.put("status","ERROR");
				out.put("message","OpenAI API error (CSV chunk " + chunk_index + "): " + full_err_c);
				info out;
				return;
			}
			// legacy output_text
			if(rm_csv != null && rm_csv.containsKey("output_text") && rm_csv.get("output_text") != null)
			{
				chunk_output_single = rm_csv.get("output_text").toString();
				info "Using rm_csv.output_text as chunk_output for chunk " + chunk_index + ".";
			}
			// new format: output[].content[].text
			if((chunk_output_single == "" || chunk_output_single == null) && rm_csv != null && rm_csv.containsKey("output"))
			{
				out_list = rm_csv.get("output");
				info "Attempting to read text from rm_csv.output[].content[].text for chunk " + chunk_index + ".";
				if(out_list != null)
				{
					for each  out_item in out_list
					{
						if(out_item != null && out_item.containsKey("content"))
						{
							cont_list = out_item.get("content");
							if(cont_list != null)
							{
								for each  c in cont_list
								{
									if(c != null && c.containsKey("text") && c.get("text") != null)
									{
										chunk_output_single = c.get("text").toString();
										break;
									}
								}
							}
						}
					}
				}
			}
		}
		catch (eparse_csv)
		{
			info "Parse note (CSV response, chunk " + chunk_index + "): " + eparse_csv;
		}
		if(chunk_output_single == null)
		{
			chunk_output_single = "";
		}
		chunk_output_single = chunk_output_single.trim();
		// strip ``` fences if present
		if(chunk_output_single.startsWith("```"))
		{
			if(chunk_output_single.length() > 3)
			{
				chunk_output_single = chunk_output_single.substring(3);
			}
			if(chunk_output_single.startsWith("\n"))
			{
				chunk_output_single = chunk_output_single.substring(1);
			}
		}
		last_fence = chunk_output_single.lastIndexOf("```");
		if(last_fence != -1)
		{
			chunk_output_single = chunk_output_single.substring(0,last_fence);
		}
		chunk_output_single = chunk_output_single.trim();
		// Remove leading 'csv' line if present
		csv_lines_tmp = chunk_output_single.toList("\n");
		if(csv_lines_tmp != null && csv_lines_tmp.size() > 0)
		{
			first_line_tmp = csv_lines_tmp.get(0);
			if(first_line_tmp != null && first_line_tmp.toLowerCase().trim() == "csv")
			{
				info "Removing leading 'csv' line from chunk " + chunk_index + ".";
				rebuilt = "";
				idx_ls = 0;
				for each  ln_tmp in csv_lines_tmp
				{
					if(idx_ls > 0)
					{
						if(rebuilt == "")
						{
							rebuilt = ln_tmp;
						}
						else
						{
							rebuilt = rebuilt + "\n" + ln_tmp;
						}
					}
					idx_ls = idx_ls + 1;
				}
				chunk_output_single = rebuilt.trim();
			}
		}
		info "chunk_output (chunk " + chunk_index + ") after fence/CSV strip, length=" + chunk_output_single.length();
		if(chunk_output_single.length() > 400)
		{
			info "chunk_output preview (chunk " + chunk_index + ", first up to 400 chars): " + chunk_output_single.substring(0,400);
		}
		else
		{
			info "chunk_output preview (chunk " + chunk_index + ", full): " + chunk_output_single;
		}
		// If empty, just skip this chunk (may contain only header/summary rows)
		if(chunk_output_single == "")
		{
			info "Note: OpenAI returned empty CSV data for chunk " + chunk_index + ". Raw: " + resp_text_csv;
		}
		else
		{
			if(chunk_output_single.startsWith("ERROR:"))
			{
				info "OpenAI returned explicit ERROR line for CSV chunk " + chunk_index + ": " + chunk_output_single;
				out.put("status","ERROR");
				out.put("message",chunk_output_single);
				info out;
				return;
			}
			// Append this chunk's rows (no header expected from model)
			if(merged_output == "")
			{
				merged_output = chunk_output_single;
			}
			else
			{
				merged_output = merged_output + "\n" + chunk_output_single;
			}
		}
	}
	// end for each chunk
	// Prepend standard header row to merged output
	export_header = "employeeid,firstname,surname,description,amount,rate,weekending,unit";
	if(merged_output == "")
	{
		chunk_output = export_header;
	}
	else
	{
		chunk_output = export_header + "\n" + merged_output;
	}
	// Filter out rows where amount or rate is 0 before writing final output
	clean_lines = List();
	for each  row_line in merged_output.toList("\n")
	{
		cols = row_line.toList(",");
		if(cols.size() == 8)
		{
			amt = ifnull(cols.get(4),"").trim();
			rt = ifnull(cols.get(5),"").trim();
			if(amt != "0" && rt != "0" && amt != "" && rt != "")
			{
				clean_lines.add(row_line);
			}
		}
	}
	chunk_output = export_header;
	for each  line in clean_lines
	{
		chunk_output = chunk_output + "\n" + line;
	}
	info "All CSV chunks processed and merged. Final CSV length: " + chunk_output.length();
	// =========================================
	// VALIDATION STEP: Check output against input rules
	// =========================================
	info "=== Starting Output Validation ===";
	// Build lookup from original input data
	input_lookup = Map();
	input_header_cols = header_row.toList(",");
	// Find column indices in input
	idx_refno = -1;
	idx_weekending = -1;
	idx_std_hrs = -1;
	idx_ot1_hrs = -1;
	idx_ot2_hrs = -1;
	idx_ot3_hrs = -1;
	idx_expenses = -1;
	idx_ot1_rate = -1;
	col_idx = 0;
	for each  hcol in input_header_cols
	{
		hcol_lower = hcol.toLowerCase().trim();
		hcol_clean = hcol.replaceAll("[^a-zA-Z0-9 ]","").toLowerCase().trim();
		if(hcol_lower.contains("candidate refno") || hcol_clean.contains("candidate refno"))
		{
			idx_refno = col_idx;
		}
		if(hcol_lower.contains("weekending"))
		{
			idx_weekending = col_idx;
		}
		if(hcol_lower.contains("std") && hcol_lower.contains("hrs"))
		{
			idx_std_hrs = col_idx;
		}
		if(hcol_lower.contains("ot1") && (hcol_lower.contains("hrs") || hcol_lower.contains("hr")) && hcol_lower.contains("rate") == false)
		{
			idx_ot1_hrs = col_idx;
		}
		if(hcol_lower.contains("ot2") && (hcol_lower.contains("hrs") || hcol_lower.contains("hr")) && hcol_lower.contains("rate") == false)
		{
			idx_ot2_hrs = col_idx;
		}
		if(hcol_lower.contains("ot3") && (hcol_lower.contains("hrs") || hcol_lower.contains("hr")) && hcol_lower.contains("rate") == false)
		{
			idx_ot3_hrs = col_idx;
		}
		if(hcol_lower.contains("expenses") && hcol_lower.contains("rate") == false)
		{
			idx_expenses = col_idx;
		}
		if(hcol_lower == "ot1 rate" || hcol_lower.contains("ot1 rate"))
		{
			idx_ot1_rate = col_idx;
		}
		col_idx = col_idx + 1;
	}
	info "Input column indices - RefNo:" + idx_refno + " Weekending:" + idx_weekending + " StdHrs:" + idx_std_hrs + " OT1Hrs:" + idx_ot1_hrs + " Expenses:" + idx_expenses + " OT1Rate:" + idx_ot1_rate;
	// Build input lookup map keyed by refno + weekending
	for each  data_row in data_rows
	{
		dcols = data_row.toList(",");
		if(dcols.size() > idx_refno && dcols.size() > idx_weekending && idx_refno >= 0 && idx_weekending >= 0)
		{
			d_refno = dcols.get(idx_refno).trim();
			d_weekending_raw = dcols.get(idx_weekending).trim();
			// Convert weekending to YYYY-MM-DD for matching
			d_weekending = d_weekending_raw;
			if(d_weekending_raw.contains("/"))
			{
				we_parts = d_weekending_raw.toList("/");
				if(we_parts.size() == 3)
				{
					we_day = we_parts.get(0);
					we_month = we_parts.get(1);
					we_year = we_parts.get(2);
					if(we_year.length() == 2)
					{
						we_year = "20" + we_year;
					}
					if(we_day.length() == 1)
					{
						we_day = "0" + we_day;
					}
					if(we_month.length() == 1)
					{
						we_month = "0" + we_month;
					}
					d_weekending = we_year + "-" + we_month + "-" + we_day;
				}
			}
			lookup_key = d_refno + "|" + d_weekending;
			input_vals = Map();
			input_vals.put("refno", d_refno);
			input_vals.put("weekending", d_weekending);
			if(idx_std_hrs >= 0 && dcols.size() > idx_std_hrs)
			{
				input_vals.put("std_hrs", dcols.get(idx_std_hrs).trim());
			}
			if(idx_ot1_hrs >= 0 && dcols.size() > idx_ot1_hrs)
			{
				input_vals.put("ot1_hrs", dcols.get(idx_ot1_hrs).trim());
			}
			if(idx_ot2_hrs >= 0 && dcols.size() > idx_ot2_hrs)
			{
				input_vals.put("ot2_hrs", dcols.get(idx_ot2_hrs).trim());
			}
			if(idx_ot3_hrs >= 0 && dcols.size() > idx_ot3_hrs)
			{
				input_vals.put("ot3_hrs", dcols.get(idx_ot3_hrs).trim());
			}
			if(idx_expenses >= 0 && dcols.size() > idx_expenses)
			{
				input_vals.put("expenses", dcols.get(idx_expenses).trim());
			}
			if(idx_ot1_rate >= 0 && dcols.size() > idx_ot1_rate)
			{
				input_vals.put("ot1_rate", dcols.get(idx_ot1_rate).trim());
			}
			input_lookup.put(lookup_key, input_vals);
		}
	}
	info "Built input lookup with " + input_lookup.size() + " entries";
	// Validate each output line
	validation_exceptions = List();
	validated_lines = List();
	for each  out_line in clean_lines
	{
		out_cols = out_line.toList(",");
		if(out_cols.size() != 8)
		{
			validated_lines.add(out_line);
			continue;
		}
		out_empid = out_cols.get(0).trim();
		out_firstname = out_cols.get(1).trim();
		out_surname = out_cols.get(2).trim();
		out_desc = out_cols.get(3).trim();
		out_amount_str = out_cols.get(4).trim();
		out_rate_str = out_cols.get(5).trim();
		out_weekending = out_cols.get(6).trim();
		out_unit = out_cols.get(7).trim();
		// Parse amount and rate as numbers
		out_amount = 0.0;
		out_rate = 0.0;
		try 
		{
			out_amount = out_amount_str.toDecimal();
		}
		catch (e_amt)
		{
			out_amount = 0.0;
		}
		try 
		{
			out_rate = out_rate_str.toDecimal();
		}
		catch (e_rt)
		{
			out_rate = 0.0;
		}
		// Find matching input row
		lookup_key = out_empid + "|" + out_weekending;
		input_data = input_lookup.get(lookup_key);
		is_valid = true;
		exception_reason = "";
		if(input_data != null)
		{
			// Parse input values
			inp_ot1_hrs = 0.0;
			inp_ot2_hrs = 0.0;
			inp_ot3_hrs = 0.0;
			inp_expenses = 0.0;
			inp_ot1_rate = 0.0;
			try 
			{
				inp_ot1_hrs_str = input_data.get("ot1_hrs");
				if(inp_ot1_hrs_str != null && inp_ot1_hrs_str != "")
				{
					inp_ot1_hrs = inp_ot1_hrs_str.toDecimal();
				}
			}
			catch (e1)
			{
				inp_ot1_hrs = 0.0;
			}
			try 
			{
				inp_ot2_hrs_str = input_data.get("ot2_hrs");
				if(inp_ot2_hrs_str != null && inp_ot2_hrs_str != "")
				{
					inp_ot2_hrs = inp_ot2_hrs_str.toDecimal();
				}
			}
			catch (e2)
			{
				inp_ot2_hrs = 0.0;
			}
			try 
			{
				inp_ot3_hrs_str = input_data.get("ot3_hrs");
				if(inp_ot3_hrs_str != null && inp_ot3_hrs_str != "")
				{
					inp_ot3_hrs = inp_ot3_hrs_str.toDecimal();
				}
			}
			catch (e3)
			{
				inp_ot3_hrs = 0.0;
			}
			try 
			{
				inp_expenses_str = input_data.get("expenses");
				if(inp_expenses_str != null && inp_expenses_str != "")
				{
					inp_expenses = inp_expenses_str.toDecimal();
				}
			}
			catch (e4)
			{
				inp_expenses = 0.0;
			}
			try 
			{
				inp_ot1_rate_str = input_data.get("ot1_rate");
				if(inp_ot1_rate_str != null && inp_ot1_rate_str != "")
				{
					inp_ot1_rate = inp_ot1_rate_str.toDecimal();
				}
			}
			catch (e5)
			{
				inp_ot1_rate = 0.0;
			}
			// VALIDATION RULE 1: OT1 line should not exist if input OT1 Hrs = 0
			if(out_desc.toLowerCase().startsWith("ot1 hrs"))
			{
				if(inp_ot1_hrs == 0 || inp_ot1_hrs == 0.0)
				{
					is_valid = false;
					exception_reason = "OT1 line created but input OT1 Hrs = 0 (OT1 Rate was " + inp_ot1_rate + ")";
				}
			}
			// VALIDATION RULE 2: OT2 line should not exist if input OT2 Hrs = 0
			if(out_desc.toLowerCase().startsWith("ot2 hrs"))
			{
				if(inp_ot2_hrs == 0 || inp_ot2_hrs == 0.0)
				{
					is_valid = false;
					exception_reason = "OT2 line created but input OT2 Hrs = 0";
				}
			}
			// VALIDATION RULE 3: OT3 line should not exist if input OT3 Hrs = 0
			if(out_desc.toLowerCase().startsWith("ot3 hrs"))
			{
				if(inp_ot3_hrs == 0 || inp_ot3_hrs == 0.0)
				{
					is_valid = false;
					exception_reason = "OT3 line created but input OT3 Hrs = 0";
				}
			}
			// VALIDATION RULE 4: Expenses line should not exist if input Expenses = 0
			if(out_desc.toLowerCase().startsWith("expenses"))
			{
				if(inp_expenses == 0 || inp_expenses == 0.0)
				{
					is_valid = false;
					exception_reason = "Expenses line created but input Expenses = 0 (OT1 Rate " + inp_ot1_rate + " may have been confused as Expenses)";
				}
			}
			// VALIDATION RULE 5: Check for swapped amount/rate (amount > 100 and rate <= 10 for hours)
			if(out_unit == "hours" && out_amount > 100 && out_rate <= 10)
			{
				is_valid = false;
				exception_reason = "Amount (" + out_amount + ") and Rate (" + out_rate + ") appear to be swapped";
			}
			// VALIDATION RULE 6: Check if Expenses rate matches an OT Rate (incorrect source)
			if(out_desc.toLowerCase().startsWith("expenses") && inp_ot1_rate > 0 && out_rate == inp_ot1_rate && inp_expenses == 0)
			{
				is_valid = false;
				exception_reason = "Expenses rate (" + out_rate + ") matches OT1 Rate - wrong column used as source";
			}
		}
		if(is_valid)
		{
			validated_lines.add(out_line);
		}
		else
		{
			exc_entry = Map();
			exc_entry.put("line", out_line);
			exc_entry.put("employeeid", out_empid);
			exc_entry.put("name", out_firstname + " " + out_surname);
			exc_entry.put("description", out_desc);
			exc_entry.put("amount", out_amount_str);
			exc_entry.put("rate", out_rate_str);
			exc_entry.put("weekending", out_weekending);
			exc_entry.put("reason", exception_reason);
			validation_exceptions.add(exc_entry);
			info "VALIDATION EXCEPTION: " + out_empid + " - " + exception_reason;
		}
	}
	info "Validation complete. Valid lines: " + validated_lines.size() + ", Exceptions: " + validation_exceptions.size();
	// =========================================
	// REPROCESSING: If exceptions found, try to reprocess those specific rows
	// =========================================
	reprocess_attempted = false;
	if(validation_exceptions.size() > 0)
	{
		info "=== Attempting to reprocess " + validation_exceptions.size() + " exception rows ===";
		reprocess_attempted = true;
		// Build a focused prompt for reprocessing with explicit corrections
		reprocess_rows_text = "";
		for each  exc in validation_exceptions
		{
			exc_empid = exc.get("employeeid");
			exc_weekending = exc.get("weekending");
			// Find original input row
			for each  data_row in data_rows
			{
				if(data_row.contains(exc_empid))
				{
					if(reprocess_rows_text == "")
					{
						reprocess_rows_text = header_row + "\n" + data_row;
					}
					else
					{
						reprocess_rows_text = reprocess_rows_text + "\n" + data_row;
					}
					break;
				}
			}
		}
		if(reprocess_rows_text != "")
		{
			// Create a stricter prompt for reprocessing
			reprocess_instructions = "CRITICAL REPROCESSING REQUEST - Previous attempt had validation errors.\n\n" + "The following rows were processed incorrectly. Please reprocess them with EXTRA CARE.\n\n" + "ERRORS TO AVOID:\n" + "1. Do NOT create OT1/OT2/OT3 lines if the corresponding OT Hours column is 0 or empty.\n" + "2. Do NOT create Expenses lines if the Expenses column is 0 or empty.\n" + "3. Do NOT use OT Rate values as Expenses values - they are completely different columns.\n" + "4. Do NOT swap amount and rate values.\n\n" + "COLUMN IDENTIFICATION:\n" + "- OT1 Hrs column contains HOURS (typically 0-70)\n" + "- OT1 Rate column contains PAY RATES (can be large numbers like 35, 675, 720)\n" + "- Expenses column is SEPARATE from all Rate columns\n\n" + "For each row, ONLY create lines where the HOURS or EXPENSES value is > 0.\n" + "If OT1 Hrs = 0, do NOT create any OT1 line regardless of what OT1 Rate shows.\n\n" + "Output format: employeeid,firstname,surname,description,amount,rate,weekending,unit\n\n";
			reprocess_part_text = Map();
			reprocess_part_text.put("type", "input_text");
			reprocess_part_text.put("text", reprocess_instructions);
			reprocess_part_csv = Map();
			reprocess_part_csv.put("type", "input_text");
			reprocess_part_csv.put("text", reprocess_rows_text);
			reprocess_content_list = List();
			reprocess_content_list.add(reprocess_part_text);
			reprocess_content_list.add(reprocess_part_csv);
			reprocess_user_msg = Map();
			reprocess_user_msg.put("role", "user");
			reprocess_user_msg.put("content", reprocess_content_list);
			reprocess_input_items = List();
			reprocess_input_items.add(reprocess_user_msg);
			reprocess_body_map = Map();
			reprocess_body_map.put("model", MODEL);
			reprocess_body_map.put("input", reprocess_input_items);
			info "Calling OpenAI for reprocessing...";
			reprocess_resp = null;
			try 
			{
				reprocess_resp = invokeurl
				[
					url :"https://api.openai.com/v1/responses"
					type :POST
					body:reprocess_body_map.toString()
					headers:headers
				];
				info "Reprocessing API call complete.";
			}
			catch (e_reprocess)
			{
				info "Reprocessing API exception: " + e_reprocess;
			}
			if(reprocess_resp != null)
			{
				reprocess_text = "";
				try 
				{
					rm_reprocess = reprocess_resp.toString().toMap();
					if(rm_reprocess != null && rm_reprocess.containsKey("output"))
					{
						out_list_r = rm_reprocess.get("output");
						if(out_list_r != null)
						{
							for each  out_item_r in out_list_r
							{
								if(out_item_r != null && out_item_r.containsKey("content"))
								{
									cont_list_r = out_item_r.get("content");
									if(cont_list_r != null)
									{
										for each  c_r in cont_list_r
										{
											if(c_r != null && c_r.containsKey("text") && c_r.get("text") != null)
											{
												reprocess_text = c_r.get("text").toString();
												break;
											}
										}
									}
								}
							}
						}
					}
				}
				catch (e_parse_r)
				{
					info "Reprocess parse error: " + e_parse_r;
				}
				// Clean up reprocessed output
				if(reprocess_text != null && reprocess_text.trim() != "")
				{
					reprocess_text = reprocess_text.trim();
					if(reprocess_text.startsWith("```"))
					{
						reprocess_text = reprocess_text.substring(3);
						if(reprocess_text.startsWith("\n"))
						{
							reprocess_text = reprocess_text.substring(1);
						}
					}
					last_fence_r = reprocess_text.lastIndexOf("```");
					if(last_fence_r != -1)
					{
						reprocess_text = reprocess_text.substring(0, last_fence_r);
					}
					reprocess_text = reprocess_text.trim();
					// Remove leading 'csv' line if present
					if(reprocess_text.toLowerCase().startsWith("csv\n"))
					{
						reprocess_text = reprocess_text.substring(4);
					}
					info "Reprocessed output: " + reprocess_text;
					// Validate reprocessed lines and add valid ones
					for each  rp_line in reprocess_text.toList("\n")
					{
						rp_cols = rp_line.toList(",");
						if(rp_cols.size() == 8)
						{
							rp_empid = rp_cols.get(0).trim();
							rp_desc = rp_cols.get(3).trim();
							rp_amount_str = rp_cols.get(4).trim();
							rp_rate_str = rp_cols.get(5).trim();
							rp_weekending = rp_cols.get(6).trim();
							rp_unit = rp_cols.get(7).trim();
							// Re-validate this line
							rp_lookup_key = rp_empid + "|" + rp_weekending;
							rp_input_data = input_lookup.get(rp_lookup_key);
							rp_is_valid = true;
							if(rp_input_data != null)
							{
								rp_inp_ot1_hrs = 0.0;
								rp_inp_expenses = 0.0;
								try 
								{
									rp_inp_ot1_hrs_str = rp_input_data.get("ot1_hrs");
									if(rp_inp_ot1_hrs_str != null && rp_inp_ot1_hrs_str != "")
									{
										rp_inp_ot1_hrs = rp_inp_ot1_hrs_str.toDecimal();
									}
								}
								catch (e_rp1)
								{
									rp_inp_ot1_hrs = 0.0;
								}
								try 
								{
									rp_inp_expenses_str = rp_input_data.get("expenses");
									if(rp_inp_expenses_str != null && rp_inp_expenses_str != "")
									{
										rp_inp_expenses = rp_inp_expenses_str.toDecimal();
									}
								}
								catch (e_rp2)
								{
									rp_inp_expenses = 0.0;
								}
								if(rp_desc.toLowerCase().startsWith("ot1 hrs") && (rp_inp_ot1_hrs == 0 || rp_inp_ot1_hrs == 0.0))
								{
									rp_is_valid = false;
									info "Reprocessed line still invalid (OT1 with 0 hrs): " + rp_line;
								}
								if(rp_desc.toLowerCase().startsWith("expenses") && (rp_inp_expenses == 0 || rp_inp_expenses == 0.0))
								{
									rp_is_valid = false;
									info "Reprocessed line still invalid (Expenses with 0): " + rp_line;
								}
								// Check for swapped values
								try 
								{
									rp_amount = rp_amount_str.toDecimal();
									rp_rate = rp_rate_str.toDecimal();
									if(rp_unit == "hours" && rp_amount > 100 && rp_rate <= 10)
									{
										rp_is_valid = false;
										info "Reprocessed line still invalid (swapped values): " + rp_line;
									}
								}
								catch (e_rp3)
								{
									// Ignore parse errors
								}
							}
							if(rp_is_valid)
							{
								// Remove the original exception and add valid reprocessed line
								validated_lines.add(rp_line);
								// Remove from exceptions list
								new_exceptions = List();
								for each  old_exc in validation_exceptions
								{
									if(old_exc.get("employeeid") != rp_empid)
									{
										new_exceptions.add(old_exc);
									}
								}
								validation_exceptions = new_exceptions;
							}
						}
					}
				}
			}
		}
		info "After reprocessing - Valid lines: " + validated_lines.size() + ", Remaining exceptions: " + validation_exceptions.size();
	}
	// =========================================
	// EMAIL EXCEPTIONS: If still have exceptions after reprocessing, email them
	// =========================================
	if(validation_exceptions.size() > 0)
	{
		info "=== Sending exception report email ===";
		email_body = "<html><body>";
		email_body = email_body + "<h2>Timesheet Conversion Validation Exceptions</h2>";
		email_body = email_body + "<p>The following rows failed validation after processing and reprocessing attempt:</p>";
		email_body = email_body + "<p><strong>Source File:</strong> " + file_name + "</p>";
		email_body = email_body + "<table border='1' cellpadding='5' cellspacing='0' style='border-collapse: collapse;'>";
		email_body = email_body + "<tr style='background-color: #f0f0f0;'>";
		email_body = email_body + "<th>Employee ID</th><th>Name</th><th>Description</th><th>Amount</th><th>Rate</th><th>Weekending</th><th>Exception Reason</th>";
		email_body = email_body + "</tr>";
		for each  exc in validation_exceptions
		{
			email_body = email_body + "<tr>";
			email_body = email_body + "<td>" + exc.get("employeeid") + "</td>";
			email_body = email_body + "<td>" + exc.get("name") + "</td>";
			email_body = email_body + "<td>" + exc.get("description") + "</td>";
			email_body = email_body + "<td>" + exc.get("amount") + "</td>";
			email_body = email_body + "<td>" + exc.get("rate") + "</td>";
			email_body = email_body + "<td>" + exc.get("weekending") + "</td>";
			email_body = email_body + "<td style='color: red;'>" + exc.get("reason") + "</td>";
			email_body = email_body + "</tr>";
		}
		email_body = email_body + "</table>";
		email_body = email_body + "<p><em>These lines have been excluded from the output file. Please review and correct manually if needed.</em></p>";
		email_body = email_body + "</body></html>";
		try 
		{
			sendmail
			[
				from :zoho.adminuserid
				to :"chris.lindsay@blossombi.com"
				subject :"Timesheet Conversion Exceptions - " + file_name
				message :email_body
				content type :HTML
			];
			info "Exception email sent to chris.lindsay@blossombi.com";
		}
		catch (e_email)
		{
			info "Failed to send exception email: " + e_email;
		}
	}
	// Rebuild chunk_output with only validated lines
	chunk_output = export_header;
	for each  v_line in validated_lines
	{
		chunk_output = chunk_output + "\n" + v_line;
	}
	info "Final output after validation - lines: " + validated_lines.size();
	// ===== Save CSV file and upload to WorkDrive =====
	csv_name2 = file_name;
	ln2 = csv_name2.toLowerCase();
	if(ln2.endsWith(".pdf"))
	{
		csv_name2 = csv_name2.substring(0,csv_name2.length() - 4) + ".csv";
	}
	else if(ln2.endsWith(".xlsx"))
	{
		csv_name2 = csv_name2.substring(0,csv_name2.length() - 5) + ".csv";
	}
	else if(ln2.endsWith(".xls"))
	{
		csv_name2 = csv_name2.substring(0,csv_name2.length() - 4) + ".csv";
	}
	else if(ln2.endsWith(".csv") == false)
	{
		csv_name2 = csv_name2 + ".csv";
	}
	info "Writing CSV to file (CSV input): " + csv_name2;
	csv_file_final = chunk_output.toFile(csv_name2);
	uploaded_file_id = "";
	uploaded_file_name = "";
	if(csv_file_final != null)
	{
		info "Uploading CSV to WorkDrive folder: " + TARGET_WORKDRIVE_FOLDER_ID;
		wd_upload_resp = null;
		try 
		{
			wd_upload_resp = zoho.workdrive.uploadFile(csv_file_final,TARGET_WORKDRIVE_FOLDER_ID,csv_name2,true,"zoho_drive");
			info "WorkDrive upload response (CSV): " + wd_upload_resp;
			if(wd_upload_resp != null && wd_upload_resp.containsKey("data"))
			{
				up_d = wd_upload_resp.get("data");
				if(up_d != null)
				{
					if(up_d.containsKey("id") && up_d.get("id") != null)
					{
						uploaded_file_id = up_d.get("id").toString();
					}
					if(up_d.containsKey("attributes"))
					{
						up_a = up_d.get("attributes");
						if(up_a != null && up_a.containsKey("name") && up_a.get("name") != null)
						{
							uploaded_file_name = up_a.get("name").toString();
						}
					}
				}
			}
		}
		catch (e_up_csv)
		{
			info "WorkDrive upload exception (CSV): " + e_up_csv;
		}
	}
	info "OpenAI CSV transformation completed successfully; returning CSV.";
	out.put("status","SUCCESS");
	out.put("message","CSV created successfully.");
	out.put("csv_text",chunk_output);
	out.put("csv_file",csv_file_final);
	if(uploaded_file_id != "")
	{
		out.put("uploaded_file_id",uploaded_file_id);
	}
	if(uploaded_file_name != "")
	{
		out.put("uploaded_file_name",uploaded_file_name);
	}
	// === NEW: expose file_name + file_content for API / Flow ===
	out.put("file_name",csv_name2);
	out.put("file_content",chunk_output);
	info out;
	return;
}
// ---------- PDF FLOW ----------
if(ext == "pdf")
{
	info "Branch selected: PDF";
	part_text2 = Map();
	part_text2.put("type","input_text");
	part_text2.put("text",instructions);
	part_file = Map();
	part_file.put("type","input_file");
	b64_clean = b64.replaceAll("\r","").replaceAll("\n","");
	part_file.put("filename",file_name);
	part_file.put("file_data","data:" + ct + ";base64," + b64_clean);
	info "Using PDF file_data (base64) for OpenAI.";
	content_list2 = List();
	content_list2.add(part_text2);
	content_list2.add(part_file);
	user_msg2 = Map();
	user_msg2.put("role","user");
	user_msg2.put("content",content_list2);
	input_items2 = List();
	input_items2.add(user_msg2);
	body_map2 = Map();
	body_map2.put("model",MODEL);
	body_map2.put("input",input_items2);
	body_map2.put("temperature",0);
	info "Calling OpenAI Responses API (PDF)...";
	resp2 = null;
	try 
	{
		resp2 = invokeurl
		[
			url :"https://api.openai.com/v1/responses"
			type :POST
			body:body_map2.toString()
			headers:headers
		];
		info "Responses API call complete.";
	}
	catch (e_resp2)
	{
		info "Responses API exception (PDF): " + e_resp2;
		out.put("status","ERROR");
		out.put("message","Responses API exception: " + e_resp2);
		info out;
		return;
	}
	if(resp2 == null)
	{
		info "ERROR: Empty response from OpenAI (PDF).";
		out.put("status","ERROR");
		out.put("message","Empty response from OpenAI.");
		info out;
		return;
	}
	resp_text2 = resp2.toString();
	if(resp_text2.length() > 500)
	{
		info "Responses API raw (PDF, first 500 chars): " + resp_text2.substring(0,500);
	}
	else
	{
		info "Responses API raw (PDF, full): " + resp_text2;
	}
	got_text2 = "";
	try 
	{
		rm2 = resp_text2.toMap();
		if(rm2 != null && rm2.containsKey("error") && rm2.get("error") != null)
		{
			err_obj2 = rm2.get("error");
			err_msg2 = "";
			err_type2 = "";
			err_code2 = "";
			if(err_obj2 != null)
			{
				if(err_obj2.containsKey("message") && err_obj2.get("message") != null)
				{
					err_msg2 = err_obj2.get("message").toString();
				}
				if(err_obj2.containsKey("type") && err_obj2.get("type") != null)
				{
					err_type2 = err_obj2.get("type").toString();
				}
				if(err_obj2.containsKey("code") && err_obj2.get("code") != null)
				{
					err_code2 = err_obj2.get("code").toString();
				}
			}
			full_err2 = err_msg2;
			if(err_type2 != "")
			{
				full_err2 = full_err2 + " (type: " + err_type2 + ")";
			}
			if(err_code2 != "")
			{
				full_err2 = full_err2 + " [code: " + err_code2 + "]";
			}
			if(full_err2 == "" || full_err2 == null)
			{
				full_err2 = resp_text2;
			}
			info "OpenAI error detected (PDF): " + full_err2;
			out.put("status","ERROR");
			out.put("message","OpenAI API error: " + full_err2);
			info out;
			return;
		}
		if(rm2 != null && rm2.containsKey("output_text") && rm2.get("output_text") != null)
		{
			got_text2 = rm2.get("output_text").toString();
			info "Using rm2.output_text for PDF.";
		}
		if((got_text2 == "" || got_text2 == null) && rm2 != null && rm2.containsKey("output"))
		{
			out_list2 = rm2.get("output");
			info "Attempting to read text from rm2.output[].content[].text (PDF).";
			if(out_list2 != null)
			{
				for each  out_item2 in out_list2
				{
					if(out_item2 != null && out_item2.containsKey("content"))
					{
						cont_list2 = out_item2.get("content");
						if(cont_list2 != null)
						{
							for each  c2 in cont_list2
							{
								if(c2 != null && c2.containsKey("text") && c2.get("text") != null)
								{
									got_text2 = c2.get("text").toString();
									break;
								}
							}
						}
					}
				}
			}
		}
	}
	catch (eparse2_main)
	{
		info "Parse note (PDF response): " + eparse2_main;
	}
	if(got_text2 == null)
	{
		got_text2 = "";
	}
	clean2 = got_text2.trim();
	if(clean2.startsWith("```"))
	{
		if(clean2.length() > 3)
		{
			clean2 = clean2.substring(3);
		}
		if(clean2.startsWith("\n"))
		{
			clean2 = clean2.substring(1);
		}
	}
	last_fence2 = clean2.lastIndexOf("```");
	if(last_fence2 != -1)
	{
		clean2 = clean2.substring(0,last_fence2);
	}
	clean2 = clean2.trim();
	info "PDF clean output length: " + clean2.length();
	if(clean2.length() > 400)
	{
		info "PDF clean output preview (first up to 400 chars): " + clean2.substring(0,400);
	}
	else
	{
		info "PDF clean output preview (full): " + clean2;
	}
	if(clean2 == "")
	{
		info "ERROR: OpenAI returned no output_text/text for PDF.";
		out.put("status","ERROR");
		out.put("message","OpenAI did not return any CSV data for PDF. Raw response logged.");
		info out;
		return;
	}
	if(clean2.startsWith("ERROR:"))
	{
		info "OpenAI returned explicit ERROR line for PDF: " + clean2;
		out.put("status","ERROR");
		out.put("message",clean2);
		info out;
		return;
	}
	// ===== Save CSV file and upload to WorkDrive (PDF  CSV) =====
	csv_name3 = file_name;
	ln3 = csv_name3.toLowerCase();
	if(ln3.endsWith(".pdf"))
	{
		csv_name3 = csv_name3.substring(0,csv_name3.length() - 4) + ".csv";
	}
	else if(ln3.endsWith(".xlsx"))
	{
		csv_name3 = csv_name3.substring(0,csv_name3.length() - 5) + ".csv";
	}
	else if(ln3.endsWith(".xls"))
	{
		csv_name3 = csv_name3.substring(0,csv_name3.length() - 4) + ".csv";
	}
	else if(ln3.endsWith(".csv") == false)
	{
		csv_name3 = csv_name3 + ".csv";
	}
	info "Writing CSV to file (PDF input): " + csv_name3;
	csv_file2 = clean2.toFile(csv_name3);
	uploaded_file_id2 = "";
	uploaded_file_name2 = "";
	if(csv_file2 != null)
	{
		info "Uploading CSV (from PDF) to WorkDrive folder: " + TARGET_WORKDRIVE_FOLDER_ID;
		wd_upload_resp2 = null;
		try 
		{
			wd_upload_resp2 = zoho.workdrive.uploadFile(csv_file2,TARGET_WORKDRIVE_FOLDER_ID,csv_name3,true,"zoho_drive");
			info "WorkDrive upload response (PDFCSV): " + wd_upload_resp2;
			if(wd_upload_resp2 != null && wd_upload_resp2.containsKey("data"))
			{
				up_d2 = wd_upload_resp2.get("data");
				if(up_d2 != null)
				{
					if(up_d2.containsKey("id") && up_d2.get("id") != null)
					{
						uploaded_file_id2 = up_d2.get("id").toString();
					}
					if(up_d2.containsKey("attributes"))
					{
						up_a2 = up_d2.get("attributes");
						if(up_a2 != null && up_a2.containsKey("name") && up_a2.get("name") != null)
						{
							uploaded_file_name2 = up_a2.get("name").toString();
						}
					}
				}
			}
		}
		catch (e_up_pdf)
		{
			info "WorkDrive upload exception (PDFCSV): " + e_up_pdf;
		}
	}
	info "OpenAI PDF transformation completed successfully; returning CSV.";
	out.put("status","SUCCESS");
	out.put("message","CSV created successfully.");
	out.put("csv_text",clean2);
	out.put("csv_file",csv_file2);
	if(uploaded_file_id2 != "")
	{
		out.put("uploaded_file_id",uploaded_file_id2);
	}
	if(uploaded_file_name2 != "")
	{
		out.put("uploaded_file_name",uploaded_file_name2);
	}
	// === NEW: expose file_name + file_content for API / Flow ===
	out.put("file_name",csv_name3);
	out.put("file_content",clean2);
	info out;
	return;
}
// Fallback catch-all
out.put("status","ERROR");
out.put("message","Unhandled file type or branch.");
info out;
return;
}
